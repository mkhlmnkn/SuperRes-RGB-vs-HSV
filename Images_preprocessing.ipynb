{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Images_preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "00Ah2XmWwDVX"
      },
      "cell_type": "markdown",
      "source": [
        "### Beginning"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7I6YCH-wxHWV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YPwjA-ZTxGtv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py, os, glob, re, itertools, datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IE7hcqaaxGt0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/My Drive/DL/datasets/DIV2K/train/\"\n",
        "valid_dir = \"/content/drive/My Drive/DL/datasets/DIV2K/valid/\"\n",
        "hdf_directory = \"/content/drive/My Drive/DL/datasets/DIV2K HDF/\"\n",
        "patched_hdf_directory = \"/content/drive/My Drive/DL/datasets/DIV2K HDF PATCHED/\"\n",
        "dir_type = { 0:\"HR/\", 1:\"LR_bicub_X2/\", 2:\"LR_unkn_X2/\" }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1AYGNne9Q2wk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "current_time = lambda: datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\")\n",
        "\n",
        "def get_hdf_dir(training, hsv, patching):\n",
        "    dataset_type = \"train/\" if training == True else \"valid/\"\n",
        "    color_model = \"RGB/\" if hsv == False else \"HSV/\" \n",
        "    dir_ = patched_hdf_directory if patching == True else hdf_directory\n",
        "    return \"{}{}{}\".format(dir_, color_model, dataset_type)\n",
        "\n",
        "def paths_gen(training):\n",
        "    if training == True:\n",
        "        directory = train_dir\n",
        "    else: directory = valid_dir\n",
        "    hr_paths = glob.glob(os.path.join(directory, dir_type[0]) + \"*.png\")\n",
        "    for i in range(len(hr_paths)):\n",
        "        path_changer = re.sub(\".png\", \"x2.png\", hr_paths[i])\n",
        "        lr_bicub = re.sub(dir_type[0], dir_type[1], path_changer)\n",
        "        lr_unkn = re.sub(dir_type[0], dir_type[2], path_changer)\n",
        "        yield hr_paths[i], lr_bicub, lr_unkn  \n",
        "\n",
        "def path2image(img_path, hsv):\n",
        "    img_raw = tf.read_file(img_path)\n",
        "    img_dec = tf.image.decode_png(img_raw, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img_dec, tf.float32).eval()\n",
        "    if hsv == True:\n",
        "            img = tf.image.rgb_to_hsv(img).eval()\n",
        "    return img\n",
        "  \n",
        "def patching(hr_img, lr_bicub_img, lr_unkn_img, patch_size):\n",
        "    hr_patchsize, lr_patchsize = [1, patch_size, patch_size, 1], [1, patch_size//2, patch_size//2, 1]\n",
        "    hr_patches = tf.extract_image_patches(\n",
        "        hr_img, hr_patchsize, hr_patchsize, [1, 1, 1, 1], 'VALID').eval()\n",
        "    lr_bicub_patches = tf.extract_image_patches(\n",
        "        lr_bicub_img, lr_patchsize, lr_patchsize, [1, 1, 1, 1], 'VALID').eval()\n",
        "    lr_unkn_patches = tf.extract_image_patches(\n",
        "        lr_unkn_img, lr_patchsize, lr_patchsize, [1, 1, 1, 1], 'VALID').eval()\n",
        "    _, R, C, _ = np.asarray(hr_patches.shape)\n",
        "    hr_list, lr_bicub_list, lr_unkn_list = [], [], []\n",
        "    patched_shape = [tf.constant(R), tf.constant(C)]\n",
        "    for r, c in itertools.product(*map(range, [R, C])):\n",
        "        hr_patch = tf.reshape(hr_patches[0, r, c, :], [patch_size, patch_size, 3])\n",
        "        lr_bicub_patch = tf.reshape(lr_bicub_patches[0, r, c, :], [patch_size//2, patch_size//2, 3])\n",
        "        lr_unkn_patch = tf.reshape(lr_unkn_patches[0, r, c, :], [patch_size//2, patch_size//2, 3])\n",
        "        hr_list.append(hr_patch)\n",
        "        lr_bicub_list.append(lr_bicub_patch)\n",
        "        lr_unkn_list.append(lr_unkn_patch)\n",
        "    return hr_list, lr_bicub_list, lr_unkn_list, patched_shape\n",
        "  \n",
        "def save_img(img, filename, hsv):\n",
        "    if hsv == True:\n",
        "        img = tf.image.hsv_to_rgb(img)\n",
        "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "    img_raw = tf.image.encode_png(img).eval()\n",
        "    return tf.write_file(tf.constant(filename), img_raw) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JvZnmTgAAB9V"
      },
      "cell_type": "markdown",
      "source": [
        "### Images preprocessing"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nkgeTh9AxGuF",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAINING = True\n",
        "HSV = False #if HSV == False => RGB = True\n",
        "PATCHING = True \n",
        "PATCH_SIZE = 96 #high resolution patch size\n",
        "\n",
        "patched_shape_log = \"/content/drive/My Drive/DL/metrics/patched_shape.txt\"\n",
        "\n",
        "to_print_or_not_to_print = 1\n",
        "with open(patched_shape_log, \"w\") as txt_file: pass\n",
        "if PATCHING == True:\n",
        "    with open(patched_shape_log, \"a+\") as txt_file:\n",
        "        txt_file.write(\"PATCH_SIZE = {}\\n\\n\".format(PATCH_SIZE))\n",
        "print(\"Start: {}\\n\".format(current_time()))\n",
        "hdf_dir = get_hdf_dir(TRAINING, HSV, PATCHING)\n",
        "for hr, lr_bicub, lr_unkn in paths_gen(TRAINING):\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        hr_img = path2image(hr, HSV)\n",
        "        lr_bicub_img = path2image(lr_bicub, HSV)\n",
        "        lr_unkn_img = path2image(lr_unkn, HSV)\n",
        "        if PATCHING == True:\n",
        "            hr_img = tf.expand_dims(hr_img, 0)\n",
        "            lr_bicub_img = tf.expand_dims(lr_bicub_img, 0)\n",
        "            lr_unkn_img = tf.expand_dims(lr_unkn_img, 0)\n",
        "            hr_img, lr_bicub_img, lr_unkn_img, patched_shape = sess.run(\n",
        "                patching(hr_img, lr_bicub_img, lr_unkn_img, PATCH_SIZE))\n",
        "        image_number = re.search(r'\\d\\d\\d\\d', re.split(r\"/\", hr)[-1]).group(0)\n",
        "        with h5py.File(\"{}{}.hdf5\".format(hdf_dir, image_number), \"w\") as hdf:\n",
        "            hdf.create_dataset(\"hr\", data=np.asarray(hr_img))\n",
        "            hdf.create_dataset(\"lr_bicub\", data=np.asarray(lr_bicub_img))\n",
        "            hdf.create_dataset(\"lr_unkn\", data=np.asarray(lr_unkn_img))\n",
        "            if PATCHING == True:\n",
        "                hdf.create_dataset(\"patched_shape\", data=np.asarray(patched_shape))\n",
        "        with open(patched_shape_log, \"a+\") as txt_file:\n",
        "            txt_file.write(\"image {}... patched_shape = {}\\n\".format(\n",
        "                image_number, np.asarray(patched_shape)))       \n",
        "        if to_print_or_not_to_print % 10 == 0:\n",
        "            print(\"{}... image {} done\".format(current_time(), image_number))\n",
        "        to_print_or_not_to_print += 1\n",
        "\n",
        "        print(\"\\nEnd: {}\".format(current_time()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wpJvUA2qASol"
      },
      "cell_type": "markdown",
      "source": [
        "### Checking"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QQR6t-OR722Z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAINING = True\n",
        "HSV = False\n",
        "PATCHING = True\n",
        "\n",
        "hdf_dir = get_hdf_dir(TRAINING, HSV, PATCHING)\n",
        "\n",
        "for hdf_name in glob.glob(hdf_dir + \"*.hdf5\"):\n",
        "    img_num = re.search(r'\\d\\d\\d\\d', re.split(r\"/\", hdf_name)[-1]).group(0)\n",
        "    with tf.Session() as sess:\n",
        "        with h5py.File(hdf_name, \"r\") as hdf:\n",
        "            hr_img = hdf[\"hr\"][()]\n",
        "            lr_bicub_img = hdf[\"lr_bicub\"][()]\n",
        "            lr_unkn_img = hdf[\"lr_unkn\"][()]\n",
        "            img = save_img(\n",
        "                hr_img[150], \"/content/drive/My Drive/DL/test_img/hr_{}.png\".format(img_num), hsv=HSV)\n",
        "            sess.run(img)\n",
        "            img = save_img(\n",
        "                lr_bicub_img[150], \"/content/drive/My Drive/DL/test_img/lr_bicub_{}.png\".format(img_num), hsv=HSV)\n",
        "            sess.run(img)\n",
        "            img = save_img(\n",
        "                lr_unkn_img[150], \"/content/drive/My Drive/DL/test_img/lr_unkn_{}.png\".format(img_num), hsv=HSV)\n",
        "            sess.run(img)\n",
        "            print(\"{} done\".format(img_num))\n",
        "\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}