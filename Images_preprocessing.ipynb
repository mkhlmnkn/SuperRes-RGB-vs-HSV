{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image preproc.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7I6YCH-wxHWV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPwjA-ZTxGtv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py, os, glob, re, itertools, datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IE7hcqaaxGt0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/My Drive/DL/DIV2K/train/\"\n",
        "valid_dir = \"/content/drive/My Drive/DL/DIV2K/valid/\"\n",
        "hdf_dir = \"/content/drive/My Drive/DL/DIV2K HDF/\"\n",
        "dir_type = { 0:\"HR/\", 1:\"LR_bicub_X2/\", 2:\"LR_bicub_X3/\", 3:\"LR_bicub_X4/\", \n",
        "               4:\"LR_unkn_X2/\", 5:\"LR_unkn_X3/\", 6:\"LR_unkn_X4/\" }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1AYGNne9Q2wk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_hdf_dir(training=True, hsv=False):\n",
        "    if hsv == False:\n",
        "        cmodel = \"RGB/\"\n",
        "    else: cmodel = \"HSV/\"\n",
        "    if training == True:\n",
        "        data = \"train/\"\n",
        "    else: data = \"valid/\"\n",
        "    return os.path.join(hdf_dir, cmodel, data)\n",
        "\n",
        "def paths_gen(training=True):\n",
        "    if training == True:\n",
        "        directory = train_dir\n",
        "    else: directory = valid_dir\n",
        "    hr_paths = glob.glob(os.path.join(directory, dir_type[0]) + \"*.png\")\n",
        "    for i in range(len(hr_paths)):\n",
        "        yield hr_paths[i]\n",
        "        \n",
        "def path2image(img_path):\n",
        "    img_raw = tf.read_file(img_path)\n",
        "    img_dec = tf.image.decode_png(img_raw, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img_dec, tf.float32)\n",
        "    return img\n",
        "\n",
        "def patches_creating(img, patch_size=48):\n",
        "    patchsize = [1, patch_size, patch_size, 1]\n",
        "    patches = tf.extract_image_patches(img, patchsize, patchsize, [1, 1, 1, 1], 'VALID')\n",
        "    return patches\n",
        "    \n",
        "def postpatching(patches, patch_size=48):\n",
        "    _, R, C, _ = np.asarray(patches.shape)\n",
        "    x2_patch_size = np.asarray([patch_size, patch_size])//2\n",
        "    x4_patch_size = np.asarray([patch_size, patch_size])//4\n",
        "    hr_patches_list = [] \n",
        "    lr_patches_x2_list = [] \n",
        "    lr_patches_x4_list = []\n",
        "    patched_image_size = [tf.constant(R), tf.constant(C)]\n",
        "    for r, c in itertools.product(*map(range, [R, C])):\n",
        "        hr_patch = tf.reshape(patches[0, r, c, :], [patch_size, patch_size, 3])\n",
        "        lr_patch_x2 = tf.image.resize_images(hr_patch, x2_patch_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        lr_patch_x4 = tf.image.resize_images(hr_patch, x4_patch_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        hr_patches_list.append(hr_patch)\n",
        "        lr_patches_x2_list.append(lr_patch_x2)\n",
        "        lr_patches_x4_list.append(lr_patch_x4)\n",
        "    return hr_patches_list, lr_patches_x2_list, lr_patches_x4_list, patched_image_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nkgeTh9AxGuF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#if HSV = False => RGB = True\n",
        "TRAINING = False\n",
        "HSV = True\n",
        "PATCH_SIZE = 48\n",
        "\n",
        "#super-duper counter for save-name\n",
        "I = 1\n",
        "\n",
        "print(\"Start:\", datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\"))\n",
        "for img_path in paths_gen(training=TRAINING):\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        img = sess.run(path2image(img_path))\n",
        "        if HSV == True:\n",
        "            img = sess.run(tf.image.rgb_to_hsv(img))\n",
        "        img = tf.expand_dims(img, 0)\n",
        "        patches = sess.run(patches_creating(img, patch_size=PATCH_SIZE))\n",
        "        hr, lr_x2, lr_x4, patched_image_size = sess.run(postpatching(patches, patch_size=PATCH_SIZE))\n",
        "        loop_hdf_dir = get_hdf_dir(training=TRAINING, hsv=HSV)\n",
        "        with h5py.File(loop_hdf_dir + \"img{}.hdf5\".format(I), \"w\") as hdf:\n",
        "            hdf.create_dataset(\"hr_img{}\".format(I), data=np.asarray(hr))\n",
        "            hdf.create_dataset(\"lr_x2_img{}\".format(I), data=np.asarray(lr_x2))\n",
        "            hdf.create_dataset(\"lr_x4_img{}\".format(I), data=np.asarray(lr_x4))\n",
        "            hdf.create_dataset(\"patched_res_img{}\".format(I), data=np.asarray(patched_image_size))\n",
        "        print(\"Image {} done, time: {}\".format(re.split(r\"/\", img_path)[-1], datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\")))\n",
        "        I += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}