{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bXKUTgRTjeOo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beginning"
      ]
    },
    {
      "metadata": {
        "id": "W6xOJle04QHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bDE9_TQ6ilA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py, os, glob, re, datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PUIYZAPM6kI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hdf_directory = \"/content/drive/My Drive/DL/datasets/DIV2K HDF/\"\n",
        "patched_hdf_directory = \"/content/drive/My Drive/DL/datasets/DIV2K HDF PATCHED/\"\n",
        "\n",
        "current_time = lambda: datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\")\n",
        "\n",
        "def get_hdf_dir(training, hsv, patching):\n",
        "    dataset_type = \"train/\" if training else \"valid/\"\n",
        "    color_model = \"HSV/\" if hsv else \"RGB/\" \n",
        "    dir_ = patched_hdf_directory if patching else hdf_directory\n",
        "    return \"{}{}{}\".format(dir_, color_model, dataset_type)\n",
        "  \n",
        "def batch_gen(training, hsv, batch):\n",
        "    \"\"\"Create patched minibatches / create nonpatched pairs \"low-res\"--\"hi-res\".\n",
        "        \n",
        "        \n",
        "    Parameters\n",
        "    ----------\n",
        "        training: bool\n",
        "            If true, \"train\" directory is used; \n",
        "            if false, \"valid\" directory is used.\n",
        "        hsv: bool \n",
        "            If true, the HSV color model is used; \n",
        "            if false, the RGB color model is used.\n",
        "        epoch: int\n",
        "            If even, \"bicubic\" interpolation is used; \n",
        "            if odd, \"unknown\" interpolation is used (for low-resolution images).\n",
        "        batch: int\n",
        "            The minibatch size.\n",
        "        \n",
        "        \n",
        "    Yields\n",
        "    ------\n",
        "        I. If \"batch\" is equal to 1:\n",
        "            numpy array \n",
        "                Low-resolution image.\n",
        "            numpy array \n",
        "                High-resolution image.\n",
        "            int \n",
        "                Image number (counter).\n",
        "            \n",
        "        II. If \"batch\" is greater than 1:\n",
        "            numpy array \n",
        "                Low-resolution image patches.\n",
        "            numpy array \n",
        "                High-resolution image patches.\n",
        "            int \n",
        "                Minibatch number (counter).\n",
        "         \n",
        "         \n",
        "    \"\"\"\n",
        "    #if batch == 1, then take nonpatched image\n",
        "    if batch == 1:\n",
        "        patching = False\n",
        "    #else take patched image\n",
        "    else: patching = True\n",
        "    hdfs_path = get_hdf_dir(training, hsv, patching)\n",
        "    hdfs = sorted(glob.glob(hdfs_path + \"*.hdf5\"))\n",
        "    if training:\n",
        "        np.random.shuffle(hdfs)\n",
        "    batch_counter = 0\n",
        "    for i in range(len(hdfs)):\n",
        "        with h5py.File(hdfs[i], \"r\") as hdf:\n",
        "            if batch == 1:\n",
        "                lr = hdf[\"lr_bicub\"][()]\n",
        "                hr = hdf[\"hr\"][()]\n",
        "                lr, hr = np.expand_dims(lr, axis=0), np.expand_dims(hr, axis=0)\n",
        "                yield lr, hr, i\n",
        "            else:\n",
        "                range_ = hdf[\"patched_shape\"][()][0]*hdf[\"patched_shape\"][()][1]\n",
        "                patches_index = list(range(range_))\n",
        "                np.random.shuffle(patches_index)\n",
        "                for j in range(len(patches_index) // batch):\n",
        "                    lr, hr = [], []\n",
        "                    batch_counter += 1\n",
        "                    for k in range(batch):\n",
        "                        lr.append(hdf[\"lr_bicub\"][()][patches_index[j * batch + k]])\n",
        "                        hr.append(hdf[\"hr\"][()][patches_index[j * batch + k]])\n",
        "                    np_lr, np_hr = np.asarray(lr), np.asarray(hr)\n",
        "                    yield np_lr, np_hr, batch_counter       \n",
        "      \n",
        "def save_img(img, filename, hsv=False):\n",
        "    if hsv:\n",
        "        img = tf.image.hsv_to_rgb(img)\n",
        "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "    img_raw = tf.image.encode_png(img).eval()\n",
        "    return tf.write_file(tf.constant(filename), img_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hm3wxZpPgH50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Architecture"
      ]
    },
    {
      "metadata": {
        "id": "x-tX6S_yuFsO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_layer(x, W_shape, b_shape):\n",
        "    W = tf.get_variable(\"W\", shape=W_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
        "    b = tf.get_variable(\"b\", shape=b_shape, initializer=tf.constant_initializer(0.))\n",
        "    conv = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")\n",
        "    return tf.nn.bias_add(conv, b)\n",
        "\n",
        "\n",
        "def edsr_res_block(x, scope):\n",
        "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
        "        shortcut = x\n",
        "        with tf.variable_scope(\"conv1\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_layer(x, [5,5,FEATURE_MAPS_NUMBER,FEATURE_MAPS_NUMBER], [FEATURE_MAPS_NUMBER])\n",
        "        x = tf.nn.relu(x)\n",
        "        with tf.variable_scope(\"conv2\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_layer(x, [5,5,FEATURE_MAPS_NUMBER,FEATURE_MAPS_NUMBER], [FEATURE_MAPS_NUMBER])\n",
        "        x = x * 0.1  \n",
        "    return x + shortcut\n",
        "\n",
        "# wdsr-a\n",
        "def wdsr_res_block(x, scope, wide):\n",
        "    SCALE = 0.1\n",
        "    \n",
        "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
        "        shortcut = x\n",
        "        with tf.variable_scope(\"conv1\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_layer(x, [5,5,FEATURE_MAPS_NUMBER,wide*FEATURE_MAPS_NUMBER], \n",
        "                                  [wide*FEATURE_MAPS_NUMBER])\n",
        "        x = tf.nn.relu(x)\n",
        "        with tf.variable_scope(\"conv2\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_layer(x, [5,5,wide*FEATURE_MAPS_NUMBER,FEATURE_MAPS_NUMBER], \n",
        "                                  [FEATURE_MAPS_NUMBER])\n",
        "        x = x * SCALE  \n",
        "    return x + shortcut  \n",
        "\n",
        "# wdsr-b\n",
        "def wdsr_res_block_v2(x, scope, wide, kernel):\n",
        "    LINEAR = 0.8\n",
        "    SCALE = 0.1\n",
        "    \n",
        "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
        "        shortcut = x\n",
        "        with tf.variable_scope(\"conv1\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_weight_norm(x, kernel, [1, 1, FEATURE_MAPS_NUMBER, wide * FEATURE_MAPS_NUMBER], [wide * FEATURE_MAPS_NUMBER])\n",
        "        x = tf.nn.relu(x)\n",
        "        with tf.variable_scope(\"conv2\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_weight_norm(x, kernel, [1, 1, wide * FEATURE_MAPS_NUMBER, int(LINEAR * FEATURE_MAPS_NUMBER)], [int(LINEAR * FEATURE_MAPS_NUMBER)])\n",
        "        with tf.variable_scope(\"conv3\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_weight_norm(x, kernel, [kernel, kernel, int(LINEAR * FEATURE_MAPS_NUMBER), FEATURE_MAPS_NUMBER], [FEATURE_MAPS_NUMBER])\n",
        "        x = x * SCALE  \n",
        "    return x + shortcut \n",
        "  \n",
        "def wdsr_branch(x, kernel, wide, l_fm, scope):\n",
        "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
        "        with tf.variable_scope(\"first_conv\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            conv1 = conv_weight_norm(x, kernel, [kernel, kernel, 3, FEATURE_MAPS_NUMBER], [FEATURE_MAPS_NUMBER])   \n",
        "        #ResBlocks Stack\n",
        "        res_stack = tf.contrib.layers.repeat(conv1, RES_BLOCKS_NUMBER, wdsr_res_block_v2, \n",
        "                                                scope=\"res_stack\", wide=wide, kernel=kernel)\n",
        "        #Conv before PixelShuffle\n",
        "        with tf.variable_scope(\"before_shuffling\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            conv2 = conv_weight_norm(res_stack, kernel, [kernel, kernel, FEATURE_MAPS_NUMBER, l_fm], [l_fm])\n",
        "        #PixelShuffle\n",
        "        shuffled = tf.nn.depth_to_space(conv2, UPSCALE_F, data_format='NHWC')\n",
        "    return shuffled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68bU9Q_86o1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def edsr(x, UPSCALE_F):\n",
        "    #First Conv\n",
        "    with tf.variable_scope(\"first_conv\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv1 = conv_layer(x, [5,5,3,FEATURE_MAPS_NUMBER], [FEATURE_MAPS_NUMBER])    \n",
        "    shortcut1 = conv1\n",
        "    #ResBlocks Stack\n",
        "    res_stack = tf.contrib.layers.repeat(conv1, RES_BLOCKS_NUMBER, edsr_res_block, \n",
        "                                            scope=\"res_stack\")\n",
        "    #Conv after ResBlocks\n",
        "    with tf.variable_scope(\"after_res_block\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv2 = conv_layer(res_stack, [5,5,FEATURE_MAPS_NUMBER,FEATURE_MAPS_NUMBER], \n",
        "                                          [FEATURE_MAPS_NUMBER])\n",
        "    #Shortcut\n",
        "    res1 = 0.1 * conv2 + shortcut1\n",
        "    #Conv before PixelShuffle\n",
        "    with tf.variable_scope(\"before_shuffling\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv3 = conv_layer(res1, [5,5,FEATURE_MAPS_NUMBER,FEATURE_MAPS_NUMBER*UPSCALE_F], \n",
        "                                      [FEATURE_MAPS_NUMBER*2])\n",
        "    #PixelShuffle\n",
        "    shuffled = tf.nn.depth_to_space(conv3, UPSCALE_F, data_format='NHWC')\n",
        "    #Conv to 3 channels\n",
        "    with tf.variable_scope(\"last_conv\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv4 = conv_layer(shuffled, [5,5,FEATURE_MAPS_NUMBER/UPSCALE_F,3], [3])\n",
        "    return conv4\n",
        "\n",
        "def wdsr(x, UPSCALE_F):\n",
        "    N_FM_LAST = 3 * UPSCALE_F ** 2\n",
        "    WIDE = 6\n",
        "    \n",
        "    #Skip Branch\n",
        "    with tf.variable_scope(\"0_before_shuffling\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv0 = conv_weight_norm(x, 5, [5, 5, 3, N_FM_LAST], [N_FM_LAST])\n",
        "    shuffled_1 = tf.nn.depth_to_space(conv0, UPSCALE_F, data_format='NHWC')\n",
        "  \n",
        "    #Main Branch\n",
        "    shuffled_2 = wdsr_branch(x, 5, WIDE, N_FM_LAST, \"main_branch\")\n",
        "\n",
        "    #Sum\n",
        "    out = shuffled_1 + shuffled_2\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rRBfIyew86K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "UPSCALE_F = 2\n",
        "FEATURE_MAPS_NUMBER = 64\n",
        "RES_BLOCKS_NUMBER = 1\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, None, None, 3], \"LR_image\")\n",
        "y = tf.placeholder(tf.float32, [None, None, None, 3], \"HR_image\")\n",
        "y_net = edsr(x, UPSCALE_F)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tefFdyotgdGR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss"
      ]
    },
    {
      "metadata": {
        "id": "sWGgVZ1MfHBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l2_lambda = tf.placeholder(tf.float32, name=\"L2_lambda\")\n",
        "vars_ = [var for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)]\n",
        "regularization = tf.multiply(tf.reduce_sum([tf.nn.l2_loss(var) for var in vars_]), l2_lambda)\n",
        "\n",
        "learning_rate = tf.placeholder(tf.float32, name=\"Learning_rate\")\n",
        "mae = tf.reduce_mean(tf.abs(tf.subtract(y_net, y)))\n",
        "train_step = tf.train.AdamOptimizer(learning_rate).minimize(tf.add(mae, regularization))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZoFJug11S1Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "IOlEt8Ob1W5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAINING = True\n",
        "HSV = True\n",
        "LAST_EPOCH = 3\n",
        "LEARNING_RATE = 5e-4\n",
        "LAMBDA = 1e-3\n",
        "BATCHSIZE = 16\n",
        "\n",
        "print(\"Choose your initial epoch:\")\n",
        "INITIAL_EPOCH = int(input())\n",
        "print(\"That's a great choice!\")\n",
        "\n",
        "if HSV: \n",
        "    save_path = \"/content/drive/My Drive/DL/ckpt/SISR HSV vs RGB/edsr/SR_HSV_edsr_\"\n",
        "    log_path = \"/content/drive/My Drive/DL/metrics/SISR HSV vs RGB/loss_hsv_edsr.txt\"\n",
        "else:    \n",
        "    save_path = \"/content/drive/My Drive/DL/ckpt/SISR HSV vs RGB/edsr/SR_RGB_edsr_\"\n",
        "    log_path = \"/content/drive/My Drive/DL/metrics/SISR HSV vs RGB/loss_rgb_edsr.txt\"\n",
        "    \n",
        "    \n",
        "print(\"Start:\", current_time())\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=1001)\n",
        "    if INITIAL_EPOCH == 0:\n",
        "        with open(log_path, \"w\") as txt_file: pass\n",
        "        print(\"OK, it's a new training (without disconnect, please BlessRNG).\\n\")\n",
        "    else: \n",
        "        saver.restore(sess, \"{}{}.ckpt\".format(save_path, INITIAL_EPOCH))\n",
        "        print(\"Сontinue training from a {} epoch (without disconnect, please BlessRNG).\\n\".format(INITIAL_EPOCH))\n",
        "    for epoch in range(INITIAL_EPOCH, LAST_EPOCH):\n",
        "        #amazing robot-face condition\n",
        "        if epoch != 0 == 0 != epoch:\n",
        "            LEARNING_RATE = LEARNING_RATE / (4.5 * epoch)\n",
        "        print(\"A new epoch is begining ({})... LR = {}\".format(epoch + 1, LEARNING_RATE))\n",
        "        for img_x, img_y, num in batch_gen(TRAINING, HSV, BATCHSIZE):\n",
        "            _, loss = sess.run([ train_step, mae ], feed_dict={ x: img_x, y: img_y, learning_rate: LEARNING_RATE, l2_lambda: LAMBDA })\n",
        "            with open(log_path, \"a+\") as txt_file:\n",
        "                txt_file.write(\"time: {}... epoch {}... batch {}... loss = {:.7f}\\n\".format(current_time(), epoch + 1, num, loss))\n",
        "            if num % 2000 == 0:\n",
        "                saver.save(sess, \"{}autosave_{}.ckpt\".format(save_path, num))\n",
        "            if num % 300 == 0:\n",
        "                print(\"{}... mb {} ({})... loss = {:.7f}\".format(current_time(), num, epoch + 1, loss))\n",
        "        saver.save(sess, \"{}{}.ckpt\".format(save_path, epoch + 1))      \n",
        "    \n",
        "print(\"\\nEnd:\", current_time())    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4m5m05c1XXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Result"
      ]
    },
    {
      "metadata": {
        "id": "AAJCPk6juiGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAINING = False\n",
        "HSV = False\n",
        "UPSCALE_F = 2\n",
        "BATCHSIZE = 1\n",
        "print(\"Choose your restoring epoch:\")\n",
        "EPOCH = int(input())\n",
        "test_images = [8, 40, 52]\n",
        "\n",
        "if HSV: \n",
        "    ckpt_path = \"/content/drive/My Drive/DL/ckpt/SISR HSV vs RGB/edsr/SR_HSV_edsr_\"\n",
        "    log_path = \"/content/drive/My Drive/DL/metrics/SISR HSV vs RGB/psnr_ssim_hsv_esdr.txt\"\n",
        "    img_save_path = \"/content/drive/My Drive/DL/test_img/edsr/HSV/\"\n",
        "else:  \n",
        "    ckpt_path = \"/content/drive/My Drive/DL/ckpt/SISR HSV vs RGB/edsr/SR_RGB_edsr_\"\n",
        "    log_path = \"/content/drive/My Drive/DL/metrics/SISR HSV vs RGB/psnr_ssim_rgb_edsr.txt\"\n",
        "    img_save_path = \"/content/drive/My Drive/DL/test_img/edsr/RGB/\"\n",
        "\n",
        "\n",
        "    \n",
        "print(\"Start:\", current_time())\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, \"{}{}.ckpt\".format(ckpt_path, EPOCH))\n",
        "    get_upsc_img = edsr(x, UPSCALE_F)\n",
        "    if EPOCH == 1:\n",
        "            with open(log_path, \"w\") as txt_file: pass\n",
        "    for lr_img, orig_img, num in batch_gen(TRAINING, HSV, BATCHSIZE):\n",
        "        if num in test_images:\n",
        "            upsc_img = sess.run(get_upsc_img, feed_dict={ x : lr_img })\n",
        "            upsc_img[upsc_img > 1] = 1\n",
        "            upsc_img[upsc_img < 0] = 0\n",
        "            upsc_img_tensor, orig_img_tensor = tf.convert_to_tensor(upsc_img), tf.convert_to_tensor(orig_img)\n",
        "            ssim, psnr = tf.image.ssim(upsc_img_tensor, orig_img_tensor, max_val=1.0).eval(), tf.image.psnr(upsc_img_tensor, orig_img_tensor, max_val=1.0).eval()\n",
        "            img_saving = save_img(upsc_img[0], \"{}{}_epoch{}.png\".format(img_save_path, num + 1, EPOCH), hsv=HSV)\n",
        "            sess.run(img_saving)\n",
        "            with open(log_path, \"a+\") as txt_file:\n",
        "                txt_file.write(\"time: {}... epoch {}... image {}... psnr {:.20f}... ssim {:.20f}\\n\".format(current_time(), EPOCH, num + 1, psnr[0], ssim[0]))\n",
        "            print(\"time: {}... epoch {}... image {}... psnr {:.7f}... ssim {:.7f}\".format(current_time(), EPOCH, num + 1, psnr[0], ssim[0]))\n",
        "            if num == test_images[-1]: break\n",
        "    \n",
        "print(\"End:\", current_time())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZU8-WgDfgTih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ]
    },
    {
      "metadata": {
        "id": "ou56p6rLgVnu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def path2image_inference(img_path):\n",
        "    img_raw = tf.read_file(img_path)\n",
        "    img_dec = tf.image.decode_image(img_raw, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img_dec, tf.float32).eval()\n",
        "    return img\n",
        "  \n",
        "def save_img_inference(img, filename):\n",
        "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "    img_raw = tf.image.encode_png(img).eval()\n",
        "    return tf.write_file(tf.constant(filename), img_raw)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    #sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, \"/content/drive/My Drive/DL/ckpt/SISR HSV vs RGB/edsr/SR_RGB_edsr3.ckpt\")\n",
        "    get_upsc_img = edsr(x, UPSCALE_F)\n",
        "    for img_path in glob.glob(\"/content/drive/My Drive/DL/test_img/edsr/_inference/*\"):\n",
        "        img = path2image_inference(img_path)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        upsc_img = sess.run(get_upsc_img, feed_dict={ x : img })\n",
        "        upsc_img[upsc_img > 1] = 1\n",
        "        upsc_img[upsc_img < 0] = 0\n",
        "        new_img_path = \"{}__x2.png\".format(os.path.splitext(re.split(r\"/\", img_path)[-1])[0])\n",
        "        img_saving = save_img_inference(upsc_img[0], \"/content/drive/My Drive/DL/test_img/edsr/_inference/{}\".format(new_img_path))\n",
        "        sess.run(img_saving)\n",
        "        print(\"Image {} done.\".format(new_img_path))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJGT0SbH66VX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bicubic interpolation PSNR / SSIM"
      ]
    },
    {
      "metadata": {
        "id": "PnBKJ8zb65_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "HSV = False\n",
        "img_num = [\"0809\", \"0841\", \"0853\"]\n",
        "hr_path = lambda img_num: \"/content/drive/My Drive/DL/test_img/edsr/bicub/{}.png\".format(img_num)\n",
        "bicub_path = lambda img_num: \"/content/drive/My Drive/DL/test_img/edsr/bicub/{}_bicub.png\".format(img_num)\n",
        "\n",
        "def path2image(img_path, hsv):\n",
        "    img_raw = tf.read_file(img_path)\n",
        "    img_dec = tf.image.decode_png(img_raw, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img_dec, tf.float32).eval()\n",
        "    if hsv: img = tf.image.rgb_to_hsv(img).eval()\n",
        "    return img\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "with sess.as_default():\n",
        "    for i in range(len(img_num)):\n",
        "        hr, bicub = path2image(hr_path(img_num[i]), HSV), path2image(bicub_path(img_num[i]), HSV)\n",
        "        hr_tensor, bicub_tensor = tf.convert_to_tensor(hr), tf.convert_to_tensor(bicub)\n",
        "        ssim, psnr = tf.image.ssim(hr_tensor, bicub_tensor, max_val=1.0).eval(), tf.image.psnr(hr_tensor, bicub_tensor, max_val=1.0).eval()\n",
        "        print(\"{}: PSNR {}... SSIM {}\".format(img_num[i], psnr, ssim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97MlEsarxwYO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}