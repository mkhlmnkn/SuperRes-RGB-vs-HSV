{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SISR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "tefFdyotgdGR",
        "qZoFJug11S1Z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bXKUTgRTjeOo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beginning"
      ]
    },
    {
      "metadata": {
        "id": "W6xOJle04QHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bDE9_TQ6ilA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py, os, datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PUIYZAPM6kI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hdf_dir = \"/content/drive/My Drive/DL/DIV2K HDF/\"\n",
        "\n",
        "def get_hdf_dir(training=True, hsv=False):\n",
        "    if hsv == False: cmodel = \"RGB/\"\n",
        "    else: cmodel = \"HSV/\"\n",
        "    if training == True: data = \"train/\"\n",
        "    else: data = \"valid/\"\n",
        "    return os.path.join(hdf_dir, cmodel, data)\n",
        "  \n",
        "def gen_all_images(training, hsv):\n",
        "    if training == True: total = 800\n",
        "    else: total = 100\n",
        "    dir_ = get_hdf_dir(training, hsv)\n",
        "    for i in range(total):\n",
        "        img = dir_ + \"img{}.hdf5\".format(i + 1)\n",
        "        with h5py.File(img, \"r\") as hdf:\n",
        "            x = hdf[\"lr_x2_img{}\".format(i + 1)][()]\n",
        "            y = hdf[\"hr_img{}\".format(i + 1)][()]\n",
        "            size = hdf[\"patched_res_img{}\".format(i + 1)][()]\n",
        "            number = i\n",
        "        yield x, y, size, number\n",
        "        \n",
        "def save_img(img, filename, hsv=False):\n",
        "    if hsv == True:\n",
        "        img = tf.image.hsv_to_rgb(img)\n",
        "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "    img_raw = tf.image.encode_png(img).eval()\n",
        "    return tf.write_file(tf.constant(filename), img_raw) \n",
        "     \n",
        "def image_reconstructing(img, img_size):\n",
        "    new_img = []\n",
        "    for i in range(img_size[0]):\n",
        "        k, l = i * img_size[1], (i + 1) * img_size[1]\n",
        "        new_line = np.concatenate((img[k:l]), axis=1)\n",
        "        new_img.append(new_line)\n",
        "    new_img = np.asarray(new_img)\n",
        "    new_img_2 = np.concatenate((new_img[:]), axis=0)\n",
        "    return new_img_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hm3wxZpPgH50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Architecture"
      ]
    },
    {
      "metadata": {
        "id": "x-tX6S_yuFsO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_layer(x, W_shape, b_shape):\n",
        "    W = tf.get_variable(\"W\", shape=W_shape, initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
        "    b = tf.get_variable(\"b\", initializer=tf.constant(0.01, shape=b_shape))\n",
        "    conv = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")\n",
        "    conv_b = tf.nn.bias_add(conv, b)\n",
        "    return conv_b\n",
        "\n",
        "def res_block(x, scope):\n",
        "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
        "        shortcut = x\n",
        "        with tf.variable_scope(\"conv1\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_layer(x, [5,5,32,32], [32])\n",
        "        x = tf.nn.relu(x)\n",
        "        with tf.variable_scope(\"conv2\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_layer(x, [5,5,32,32], [32])\n",
        "        x = x * 0.1  \n",
        "    return x + shortcut"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68bU9Q_86o1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def net(x):\n",
        "    #First Conv\n",
        "    with tf.variable_scope(\"first_conv\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv1 = conv_layer(x, [5,5,3,32], [32])    \n",
        "    shortcut1 = conv1\n",
        "    #ResBlocks Stack\n",
        "    res_stack = tf.contrib.layers.repeat(conv1, 16, res_block, scope=\"res_stack\")\n",
        "    #Conv after ResBlocks\n",
        "    with tf.variable_scope(\"after_res_block\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv2 = conv_layer(res_stack, [5,5,32,32], [32])\n",
        "    #Shortcut before Shuffling\n",
        "    res1 = 0.1 * conv2 + shortcut1\n",
        "    #Conv befor Shuffling\n",
        "    with tf.variable_scope(\"before_shuffling\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv3 = conv_layer(res1, [5,5,32,64], [64])\n",
        "    #Shuffling\n",
        "    shuffled = tf.nn.depth_to_space(conv3, 2, data_format='NHWC') #maybe NCWH is better?\n",
        "    #Conv to 3 channels\n",
        "    with tf.variable_scope(\"last_conv\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv4 = conv_layer(shuffled, [5,5,16,3], [3])\n",
        "    return conv4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rRBfIyew86K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, 24, 24, 3])\n",
        "y = tf.placeholder(tf.float32, [None, 48, 48, 3])\n",
        "y_net = net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tefFdyotgdGR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss"
      ]
    },
    {
      "metadata": {
        "id": "sWGgVZ1MfHBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vars_ = [var for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)]\n",
        "regularization = tf.multiply(tf.reduce_sum([tf.nn.l2_loss(var) for var in vars_]), 0.01)\n",
        "\n",
        "learning_rate = tf.placeholder(tf.float32)\n",
        "mae = tf.reduce_mean(tf.abs(tf.subtract(y_net, y)))\n",
        "train_step = tf.train.AdamOptimizer(learning_rate).minimize(tf.add(mae, regularization))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZoFJug11S1Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "IOlEt8Ob1W5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAINING = True\n",
        "HSV = True\n",
        "BATCH = 32\n",
        "EPOCHS = 15\n",
        "\n",
        "if HSV == False: save_path = \"/content/drive/My Drive/DL/ckpt/SR_RGB_\"\n",
        "else: save_path = \"/content/drive/My Drive/DL/ckpt/SR_HSV_\"\n",
        "  \n",
        "print(\"Start:\", datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\"))\n",
        "  \n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=30)\n",
        "    for i in range(EPOCHS):\n",
        "        if i <= 5: lr = 0.00001\n",
        "        else: lr = 0.000001\n",
        "        for lr_img, hr_img, size, number in gen_all_images(training=TRAINING, hsv=HSV):\n",
        "            batch_count = (size[0] * size[1]) // BATCH\n",
        "            for j in range(batch_count):\n",
        "                k, l = (BATCH * j), BATCH * (j + 1) - 1\n",
        "                batch_x, batch_y = lr_img[k:l], hr_img[k:l]\n",
        "                _, loss = sess.run([ train_step, mae ], feed_dict={ x: batch_x, y: batch_y, learning_rate: lr })\n",
        "                if number % 100 == 0 and j == 0:\n",
        "                    print(\"time: {}... epoch {}... image {}... loss = {:.7f}\".format(datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\"), i, number, loss))\n",
        "        saver.save(sess, save_path + \"{}.ckpt\".format(i))      \n",
        "    \n",
        "print(\"End:\", datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\"))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4m5m05c1XXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Result"
      ]
    },
    {
      "metadata": {
        "id": "AAJCPk6juiGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAINING = False\n",
        "HSV = False\n",
        "print(\"Choose your epoch (0-14):\")\n",
        "EPOCH = input()\n",
        "test_images = [2, 8, 9, 11, 15, 28, 37, 41, 52, 55, 98]\n",
        "\n",
        "if HSV == False: \n",
        "    ckpt_path = \"/content/drive/My Drive/DL/ckpt/SR_RGB_\"\n",
        "    metrics_path = \"/content/drive/My Drive/DL/metrics/RGB_SR_PSNR_SSIM_\"\n",
        "    img_save_path = \"/content/drive/My Drive/DL/test_img/RGB/\"\n",
        "else: \n",
        "    ckpt_path = \"/content/drive/My Drive/DL/ckpt/SR_HSV_\"\n",
        "    metrics_path = \"/content/drive/My Drive/DL/metrics/HSV_SR_PSNR_SSIM_\"\n",
        "    img_save_path = \"/content/drive/My Drive/DL/test_img/HSV/\"\n",
        "    \n",
        "print(\"Start:\", datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\"))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, \"{}{}.ckpt\".format(ckpt_path, EPOCH))\n",
        "    get_upsc_img = net(x)\n",
        "    metrics = []\n",
        "    for lr_img, hr_img, size, number in gen_all_images(training=TRAINING, hsv=HSV):\n",
        "        if number in test_images:\n",
        "            upsc_img = sess.run(get_upsc_img, feed_dict={ x : lr_img })\n",
        "            new_img, old_img = image_reconstructing(np.asarray(upsc_img), size), image_reconstructing(np.asarray(hr_img), size)\n",
        "            new_img_t, old_img_t = tf.convert_to_tensor(new_img), tf.convert_to_tensor(old_img)\n",
        "            ssim, psnr = tf.image.ssim(new_img_t, old_img_t, max_val=1.0).eval(), tf.image.psnr(new_img_t, old_img_t, max_val=1.0).eval()\n",
        "            metrics.append((ssim, psnr))\n",
        "            print(\"time: {}... image {}\".format(datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\"), number))\n",
        "            img_saving = save_img(new_img, \"{}img{}_epoch{}.png\".format(img_save_path, number, EPOCH), hsv=HSV)\n",
        "            sess.run(img_saving)\n",
        "    metrics = np.asarray(metrics)\n",
        "    print(metrics.shape)\n",
        "    np.save(\"{}epoch_{}\".format(metrics_path, EPOCH), metrics)\n",
        "                 \n",
        "print(\"End:\", datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}