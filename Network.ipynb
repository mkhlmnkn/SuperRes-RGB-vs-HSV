{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bXKUTgRTjeOo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beginning"
      ]
    },
    {
      "metadata": {
        "id": "W6xOJle04QHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bDE9_TQ6ilA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py, os, glob, re, datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PUIYZAPM6kI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hdf_directory = \"/content/drive/My Drive/DL/datasets/DIV2K HDF/\"\n",
        "patched_hdf_directory = \"/content/drive/My Drive/DL/datasets/DIV2K HDF PATCHED/\"\n",
        "\n",
        "current_time = lambda: datetime.datetime.now().strftime(\"%m/%d %H:%M:%S\")\n",
        "\n",
        "def get_hdf_dir(training, hsv, patching):\n",
        "    dataset_type = \"train/\" if training == True else \"valid/\"\n",
        "    color_model = \"RGB/\" if hsv == False else \"HSV/\" \n",
        "    dir_ = patched_hdf_directory if patching == True else hdf_directory\n",
        "    return \"{}{}{}\".format(dir_, color_model, dataset_type)\n",
        "  \n",
        "def batch_gen(training, hsv, epoch, batch):\n",
        "    \"\"\"Create patched minibatches / create nonpatched pairs \"low-res\"--\"hi-res\".\n",
        "        \n",
        "        \n",
        "    Parameters\n",
        "    ----------\n",
        "        training: bool\n",
        "            If true, \"train\" directory is used; if false, \"valid\" directory is used.\n",
        "        hsv: bool \n",
        "            If true, the HSV color model is used; if false, the RGB color model is used.\n",
        "        epoch: int\n",
        "            If even, \"bicubic\" interpolation is used; if odd, \"unknown\" interpolation is used (for low-resolution images).\n",
        "        batch: int\n",
        "            The minibatch size.\n",
        "        \n",
        "        \n",
        "    Yields\n",
        "    ------\n",
        "        I. If \"batch\" is equal to 1:\n",
        "            numpy array \n",
        "                Low-resolution image.\n",
        "            numpy array \n",
        "                High-resolution image.\n",
        "            int \n",
        "                Image number (counter).\n",
        "            \n",
        "        II. If \"batch\" is greater than 1:\n",
        "            numpy array \n",
        "                Low-resolution image patches.\n",
        "            numpy array \n",
        "                High-resolution image patches.\n",
        "            int \n",
        "                Minibatch number (counter).\n",
        "         \n",
        "         \n",
        "    \"\"\"\n",
        "    #if batch == 1, then take nonpatched image\n",
        "    if batch == 1:\n",
        "        patching = False\n",
        "    #else take patched image\n",
        "    else: patching = True\n",
        "    hdfs_path = get_hdf_dir(training, hsv, patching)\n",
        "    hdfs = sorted(glob.glob(hdfs_path + \"*.hdf5\"))\n",
        "    if training == True:\n",
        "        np.random.shuffle(hdfs)\n",
        "    batch_counter = 0\n",
        "    for i in range(len(hdfs)):\n",
        "        with h5py.File(hdfs[i], \"r\") as hdf:\n",
        "            if batch == 1:\n",
        "                if epoch % 2 == 0:\n",
        "                    lr = hdf[\"lr_bicub\"][()]\n",
        "                if epoch % 2 == 1:\n",
        "                    lr = hdf[\"lr_unkn\"][()]\n",
        "                hr = hdf[\"hr\"][()]\n",
        "                lr, hr = np.expand_dims(lr, axis=0), np.expand_dims(hr, axis=0)\n",
        "                yield lr, hr, i\n",
        "            else:\n",
        "                patches_index = list(range(hdf[\"patched_shape\"][()][0]*hdf[\"patched_shape\"][()][1]))\n",
        "                np.random.shuffle(patches_index)\n",
        "                for j in range(len(patches_index) // batch):\n",
        "                    lr, hr = [], []\n",
        "                    batch_counter += 1\n",
        "                    for k in range(batch):\n",
        "                        if epoch % 2 == 0:\n",
        "                            lr.append(hdf[\"lr_bicub\"][()][patches_index[j * batch + k]])\n",
        "                        if epoch % 2 == 1:\n",
        "                            lr.append(hdf[\"lr_unkn\"][()][patches_index[j * batch + k]])\n",
        "                        hr.append(hdf[\"hr\"][()][patches_index[j * batch + k]])\n",
        "                    yield np.asarray(lr), np.asarray(hr), batch_counter            \n",
        "      \n",
        "def save_img(img, filename, hsv=False):\n",
        "    if hsv == True:\n",
        "        img = tf.image.hsv_to_rgb(img)\n",
        "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "    img_raw = tf.image.encode_png(img).eval()\n",
        "    return tf.write_file(tf.constant(filename), img_raw) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hm3wxZpPgH50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Architecture"
      ]
    },
    {
      "metadata": {
        "id": "x-tX6S_yuFsO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FEATURE_MAPS_NUMBER = 64\n",
        "RES_BLOCKS_NUMBER = 5\n",
        "\n",
        "def conv_layer(x, W_shape, b_shape):\n",
        "    W = tf.get_variable(\"W\", shape=W_shape, initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
        "    b = tf.get_variable(\"b\", initializer=tf.constant(0.01, shape=b_shape))\n",
        "    conv = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")\n",
        "    conv_b = tf.nn.bias_add(conv, b)\n",
        "    return conv_b\n",
        "\n",
        "def res_block(x, scope):\n",
        "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
        "        shortcut = x\n",
        "        with tf.variable_scope(\"conv1\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_layer(x, [5,5,FEATURE_MAPS_NUMBER,FEATURE_MAPS_NUMBER], [FEATURE_MAPS_NUMBER])\n",
        "        x = tf.nn.relu(x)\n",
        "        with tf.variable_scope(\"conv2\", reuse=tf.AUTO_REUSE) as scope:\n",
        "            x = conv_layer(x, [5,5,FEATURE_MAPS_NUMBER,FEATURE_MAPS_NUMBER], [FEATURE_MAPS_NUMBER])\n",
        "        x = x * 0.1  \n",
        "    return x + shortcut"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "68bU9Q_86o1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def net(x):\n",
        "    #First Conv\n",
        "    with tf.variable_scope(\"first_conv\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv1 = conv_layer(x, [5,5,3,FEATURE_MAPS_NUMBER], [FEATURE_MAPS_NUMBER])    \n",
        "    shortcut1 = conv1\n",
        "    #ResBlocks Stack\n",
        "    res_stack = tf.contrib.layers.repeat(conv1, RES_BLOCKS_NUMBER, res_block, scope=\"res_stack\")\n",
        "    #Conv after ResBlocks\n",
        "    with tf.variable_scope(\"after_res_block\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv2 = conv_layer(res_stack, [5,5,FEATURE_MAPS_NUMBER,FEATURE_MAPS_NUMBER], [FEATURE_MAPS_NUMBER])\n",
        "    #Shortcut\n",
        "    res1 = 0.1 * conv2 + shortcut1\n",
        "    #Conv before PixelShuffle\n",
        "    with tf.variable_scope(\"before_shuffling\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv3 = conv_layer(res1, [5,5,FEATURE_MAPS_NUMBER,FEATURE_MAPS_NUMBER*2], [FEATURE_MAPS_NUMBER*2])\n",
        "    #PixelShuffle\n",
        "    shuffled = tf.nn.depth_to_space(conv3, 2, data_format='NHWC') #maybe NCWH is better?\n",
        "    #Conv to 3 channels\n",
        "    with tf.variable_scope(\"last_conv\", reuse=tf.AUTO_REUSE) as scope:\n",
        "        conv4 = conv_layer(shuffled, [5,5,FEATURE_MAPS_NUMBER/2,3], [3])\n",
        "    return conv4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rRBfIyew86K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, None, None, 3])\n",
        "y = tf.placeholder(tf.float32, [None, None, None, 3])\n",
        "y_net = net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tefFdyotgdGR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss"
      ]
    },
    {
      "metadata": {
        "id": "sWGgVZ1MfHBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l2_lambda = 0.01\n",
        "\n",
        "vars_ = [var for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)]\n",
        "regularization = tf.multiply(tf.reduce_sum([tf.nn.l2_loss(var) for var in vars_]), l2_lambda)\n",
        "\n",
        "learning_rate = tf.placeholder(tf.float32)\n",
        "mae = tf.reduce_mean(tf.abs(tf.subtract(y_net, y)))\n",
        "train_step = tf.train.AdamOptimizer(learning_rate).minimize(tf.add(mae, regularization))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZoFJug11S1Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "IOlEt8Ob1W5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAINING = True\n",
        "HSV = False\n",
        "LAST_EPOCH = 12\n",
        "LEARNING_RATE = 0.0005\n",
        "BATCHSIZE = 16\n",
        "\n",
        "print(\"Choose your initial epoch:\")\n",
        "INITIAL_EPOCH = int(input())\n",
        "print(\"That's a great choice!\")\n",
        "\n",
        "if HSV == False: \n",
        "    save_path = \"/content/drive/My Drive/DL/ckpt/SR_RGB_\"\n",
        "    log_path = \"/content/drive/My Drive/DL/metrics/loss_rgb.txt\"\n",
        "else: \n",
        "    save_path = \"/content/drive/My Drive/DL/ckpt/SR_HSV_\"\n",
        "    log_path = \"/content/drive/My Drive/DL/metrics/loss_hsv.txt\"\n",
        "  \n",
        "print(\"Start:\", current_time())\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=1000)\n",
        "    if INITIAL_EPOCH == 0:\n",
        "        with open(log_path, \"w\") as txt_file: pass\n",
        "        print(\"OK, it's a new training (without disconnect, please BlessRNG).\\n\")\n",
        "    else: \n",
        "        saver.restore(sess, \"{}{}.ckpt\".format(save_path, INITIAL_EPOCH))\n",
        "        print(\"Ð¡ontinue training from a {} epoch (without disconnect, please BlessRNG).\\n\".format(INITIAL_EPOCH))\n",
        "    for epoch in range(INITIAL_EPOCH, LAST_EPOCH):\n",
        "        for img_x, img_y, num in batch_gen(TRAINING, HSV, epoch, BATCHSIZE):\n",
        "            _, loss = sess.run([ train_step, mae ], feed_dict={ x: img_x, y: img_y, learning_rate: LEARNING_RATE })\n",
        "            with open(log_path, \"a+\") as txt_file:\n",
        "                txt_file.write(\"time: {}... epoch {}... batch {}... loss = {:.7f}\\n\".format(current_time(), epoch + 1, num, loss))\n",
        "            if num % 2000 == 0:\n",
        "                print(\"time: {}... epoch {}... batch {}... loss = {:.7f}\".format(current_time(), epoch + 1, num, loss))\n",
        "        saver.save(sess, \"{}{}.ckpt\".format(save_path, epoch + 1))      \n",
        "    \n",
        "print(\"\\nEnd:\", current_time())    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4m5m05c1XXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Result"
      ]
    },
    {
      "metadata": {
        "id": "AAJCPk6juiGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAINING = False\n",
        "HSV = False\n",
        "BATCHSIZE = 1\n",
        "print(\"Choose your restoring epoch:\")\n",
        "EPOCH = int(input())\n",
        "test_images = [8, 40, 52]\n",
        "\n",
        "if HSV == False: \n",
        "    ckpt_path = \"/content/drive/My Drive/DL/ckpt/SR_RGB_\"\n",
        "    log_path = \"/content/drive/My Drive/DL/metrics/psnr_ssim_rgb.txt\"\n",
        "    img_save_path = \"/content/drive/My Drive/DL/test_img/RGB/\"\n",
        "else: \n",
        "    ckpt_path = \"/content/drive/My Drive/DL/ckpt/SR_HSV_\"\n",
        "    log_path = \"/content/drive/My Drive/DL/metrics/psnr_ssim_hsv.txt\"\n",
        "    img_save_path = \"/content/drive/My Drive/DL/test_img/HSV/\"\n",
        "    \n",
        "print(\"Start:\", current_time())\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, \"{}{}.ckpt\".format(ckpt_path, EPOCH))\n",
        "    get_upsc_img = net(x)\n",
        "    if EPOCH == 1:\n",
        "        with open(log_path, \"w\") as txt_file: pass\n",
        "    for lr_img, orig_img, num in batch_gen(TRAINING, HSV, 1, BATCHSIZE):\n",
        "        if num in test_images:\n",
        "            upsc_img = sess.run(get_upsc_img, feed_dict={ x : lr_img })\n",
        "            upsc_img[upsc_img > 1] = 1\n",
        "            upsc_img[upsc_img < 0] = 0\n",
        "            upsc_img_tensor, orig_img_tensor = tf.convert_to_tensor(upsc_img), tf.convert_to_tensor(orig_img)\n",
        "            ssim, psnr = tf.image.ssim(upsc_img_tensor, orig_img_tensor, max_val=1.0).eval(), tf.image.psnr(upsc_img_tensor, orig_img_tensor, max_val=1.0).eval()\n",
        "            img_saving = save_img(upsc_img[0], \"{}{}_epoch{}.png\".format(img_save_path, num + 1, EPOCH), hsv=HSV)\n",
        "            sess.run(img_saving)\n",
        "            with open(log_path, \"a+\") as txt_file:\n",
        "                txt_file.write(\"time: {}... epoch {}... image {}... psnr {:.7f}... ssim {:.7f}\\n\".format(current_time(), EPOCH, num + 1, psnr[0], ssim[0]))\n",
        "            print(\"time: {}... epoch {}... image {}... psnr {:.7f}... ssim {:.7f}\".format(current_time(), EPOCH, num + 1, psnr[0], ssim[0]))\n",
        "    \n",
        "print(\"End:\", current_time())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}